{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "driver = webdriver.Chrome()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# python から scrapbox に記入\n",
    "webbrowser だと tab を開いてしまうのが課題\n",
    "\n",
    "https://ich.hatenadiary.com/entry/scrapbox-newpage-from-terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "# import chromedriver_binary\n",
    "# from selenium import webdriver\n",
    "import urllib.request\n",
    "\n",
    "import time, datetime\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WriteToScrapbox():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 project: str,   # project name\n",
    "                ):\n",
    "        self.today = datetime.datetime.today().strftime(\"%Y.%m.%d\")\n",
    "        self.project = project[:-1] if project.endswith('/') else project\n",
    "        \n",
    "    def write(self, title, contents, tag, open=False):\n",
    "        tag += '\\n\\n'\n",
    "        title, contents, tag = map(lambda x: urllib.parse.quote(x), [title, contents, tag])\n",
    "        url = 'https://scrapbox.io/{}/{}?body={}{}'.format(\n",
    "                        self.project, title, tag, contents)\n",
    "        webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = 'ms-papers'\n",
    "sbwriter = WriteToScrapbox(project)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "title = sbwriter.today\n",
    "body = 'foo'\n",
    "tag = '#gg'\n",
    "\n",
    "sbwriter.write(title, body, tag, open=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import arxiv\n",
    "from googletrans import Translator\n",
    "\n",
    "\n",
    "trans = Translator()\n",
    "def translator(text, src='en', tgt='ja'):\n",
    "    return trans.translate(text, src=src, dest=tgt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping():\n",
    "    def __init__(self, url:str):\n",
    "        self.url = url\n",
    "    \n",
    "    def _request(self, url):   # -> bs4.BeautifulSoup\n",
    "        return requests.get(self.url, timeout=10)\n",
    "    \n",
    "    def _bs4(self, r):\n",
    "        return BeautifulSoup(r.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadFromAnthology(Scraping):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 conference: str,\n",
    "                 year: int,\n",
    "                 url: str = 'https://www.aclweb.org/anthology',\n",
    "                 ):\n",
    "        super().__init__(url)\n",
    "        self.conference = '{}-{}'.format(conference, year)\n",
    "        self.parent = url\n",
    "        self.url = self.parent + '/events/{}-{}/'.format(conference.lower(), year)\n",
    "        self.r = self._request(self.url)\n",
    "        self.soup = self._bs4(self.r)\n",
    "        \n",
    "    def read(self, *query) -> List[Dict[str, str]]:\n",
    "        \"\"\" WEB SOURCE\n",
    "        <span class=d-block>\n",
    "            <strong><a class=align-middle href=/anthology/W19-3202/>\n",
    "                Lexical Normalization of User-Generated Medical Text\n",
    "            </a></strong>\n",
    "        \"\"\"\n",
    "        papers = self.soup.find_all('a',\n",
    "                       attrs={'href':re.compile('/anthology/[A-Z]([0-9]*?)-([0-9]*?)/'), 'class':'align-middle'},\n",
    "                       text=re.compile('|'.join(query), flags=re.IGNORECASE)\n",
    "                      )\n",
    "        return [{'title': p.text, 'url': self.parent[:-len('/anthology')]+p['href'], 'venue':self.conference} for p in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = ReadFromAnthology('ACL', 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'The Meaning of “Most” for Visual Question Answering Models',\n",
       " 'url': 'https://www.aclweb.org/anthology/W19-4806/',\n",
       " 'venue': 'ACL-2019'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers = reader.read('semantic Role', 'QA', 'Question Answering')\n",
    "papers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReadFromAnthology.read から取得可能な paper が arxiv にヒットしない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "    \n",
    "class ArxivSearch():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def replacing(self, abst:str):\n",
    "        def handler(abst, before, after):\n",
    "            return abst.replace(before, after)\n",
    "        conversions = {'\\n':' ', 'G\\\\':'', '\\\\':'', '{':''}\n",
    "        map(lambda b, a: handler(abst, b, a), conversions.items())\n",
    "        return ' '.join(abst.split('\\n'))\n",
    "        \n",
    "    def paper_info(self, year, sort_by='submittedDate', order='descending', max=50, **query) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        sort_by: relevance, lastUpdatedDate, submittedDate\n",
    "        ------\n",
    "        ti: Title\n",
    "        au: Author\n",
    "        abs: Abstract\n",
    "        cat: Subject Category\n",
    "        all: All\n",
    "        \"\"\"\n",
    "        tmp = []\n",
    "        for k, vs in query.items():\n",
    "            if type(vs) is str:\n",
    "                tmp.append('{}:{}'.format(k, vs))\n",
    "            else:\n",
    "                for v in vs:\n",
    "                    tmp.append('{}:{}'.format(k, v))\n",
    "                    \n",
    "        q = ' OR '.join(tmp)\n",
    "        out = []\n",
    "        ls = arxiv.query(query=q, max_results=max)\n",
    "        if ls:\n",
    "            for l in ls:\n",
    "                published = l.get('published')\n",
    "                if (published is not None) and int(published[:4]) == year:\n",
    "                    en_abst = self.replacing(l.get('summary', ''))\n",
    "                    ja_abst = translator(en_abst).text\n",
    "                    info = {\n",
    "                        'title': l.get('title', ''),\n",
    "                        'author': l.get('author', ''),\n",
    "                        'arxiv': l.get('arxiv_url', ''),\n",
    "                        'en_abst': en_abst,\n",
    "                        'ja_abst': ja_abst\n",
    "                    }\n",
    "                    out.append(info)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ti:semantic role\n"
     ]
    }
   ],
   "source": [
    "arxiver = ArxivSearch()\n",
    "papers = arxiver.paper_info(ti='semantic role')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    reading_src = ['arxiv', 'anthology'][0]\n",
    "    YEAR = 2019\n",
    "    QUERY_TI = ('semantic Role', 'QA', 'Question Answering')\n",
    "    VENUE = 'ACL'\n",
    "    \n",
    "    ## scrapbox\n",
    "    project = 'ms-papers'\n",
    "    sbwriter = WriteToScrapbox(project)\n",
    "    \n",
    "    ## papers from ACLAnthology\n",
    "    if reading_src == 'anthology':\n",
    "        reader = ReadFromAnthology(VENUE, YEAR) #乱発禁止\n",
    "        papers = reader.read(*QUERY_TI)\n",
    "\n",
    "        for paper in papers:\n",
    "            tag = '#' + paper['venue']\n",
    "            title = paper['title']\n",
    "            body = paper['url']\n",
    "            sbwriter.write(title, body, tag)\n",
    "            \n",
    "    elif reading_src == 'arxiv':\n",
    "        reader = ArxivSearch()\n",
    "        papers = reader.paper_info(YEAR, ti=QUERY_TI)\n",
    "        \n",
    "        body = ''\n",
    "        for paper in papers:\n",
    "            title = paper['title']\n",
    "            body += paper['author'] + '\\n'\n",
    "            body += paper['arxiv'] + '\\n\\n'\n",
    "            body += '>' + paper['en_abst'] + '\\n\\n'\n",
    "            body += '>' + paper['ja_abst'] + '\\n\\n'\n",
    "            tag = '#' + str(YEAR)\n",
    "            sbwriter.write(title, body, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
